---
title: "New York Times API and httr"
author: "Connor Gilroy"
date: "`r Sys.Date()`"
output: 
  html_document: 
    keep_md: true
  ioslides_presentation: default
---

## Outline

- Introduce `httr`
- Introduce NYT Article Search API
- Construct a request
- Look at the response
- (Bonus: extract data from multiple responses, clean, and save to csv)

## Packages used

```{r message=FALSE, warning=FALSE}
library(httr)
library(yaml)
library(jsonlite)
library(dplyr)
```

## The `httr` package

`httr` methods map to HTTP verbs like GET and POST. 

The [quickstart guide](https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html) for `httr` is very useful. 

`httr` is very similar to the `requests` package in Python. 

## The New York Times API

http://developer.nytimes.com/

Requires an API key; signup is straightforward. 

The NYT maintains several APIs. The one I'm demoing is the Article Search API. See here for documentation: http://developer.nytimes.com/article_search_v2.json

## Why not use a dedicated package?

There are dedicated R and Python packages that serve as wrappers for the NYT API--- [`rtimes`](https://cran.r-project.org/web/packages/rtimes/index.html) for R and [`nytimesarticle`](https://pypi.python.org/pypi/nytimesarticle/0.1.0) for Python. 

But learning a more generic http requests package translates to other APIs, not just the New York Times, and you aren't relying on the wrapper package being maintained if something about the NYT API changes. You can just read the NYT documentation and update your requests yourself.  

## Load API key and base url for requests

I prefer to store my API keys in a YAML config file. I add that file to .gitignore to avoid committing it to GitHub.

```{r}
nyt_api <- yaml.load_file("nyt_api.yml")
nyt_articlesearch_url <- 
  "https://api.nytimes.com/svc/search/v2/articlesearch.json"
```

(YAML is like JSON or XML but more readable.)

## Query parameters

`httr` will stick these at the end of your request so that it looks like this:

```
http://www.example.com/something?querykey1=value1&querykey2=value2
```

```{r}
query_list <- 
  list(
    `api-key` = nyt_api$article_search_api_key, 
    fq = "kicker:(%22Modern Love%22)"
  )
```

"Modern Love" is surrounded by quotation marks; for whatever reason this works best when the quotation marks are [percent-encoded](https://en.wikipedia.org/wiki/Percent-encoding) manually. The parentheses also wind up percent-encoded in the end, so don't ask me why...

## Make a request

```{r}
r <- GET(nyt_articlesearch_url, query = query_list)
```

## Check out the response

```{r}
status_code(r)
content(r)$response$meta
docs <- content(r)$response$docs
```

----

```{r echo=FALSE}
str(docs[[1]])
```

## Create data frame with desired fields

```{r}
modern_love_from_api <- 
    lapply(docs, function(x) {
    data_frame(pub_date = x$pub_date, 
               title = x$headline$main,
               author = x$byline$original,
               snippet = x$snippet,
               web_url = x$web_url)
    }) %>% bind_rows()
```

----

```{r tidy=TRUE, tidy.opts=list(width.cutoff=75)}
modern_love_from_api$title
```

----

```{r tidy=TRUE, tidy.opts=list(width.cutoff=75)}
modern_love_from_api$snippet
```

## Pages

Each "page" of the response returns 10 articles (`content(r)$response$docs`). You can request a specific page as part of the query. 

```{r}
## make sure I got the right number of hits
hits <- content(r)$response$meta$hits
hits

## 613 hits, but only 10 per "page"
pages <- 0:(ceiling(hits/10) - 1)
```

## Bonus: get all the pages

Note the use of `Sys.sleep()` to avoid being rate-limited. (5 seconds per call is probably overkill; the [actual limit](http://developer.nytimes.com/faq#12) is 5 calls per second, max 1000 per day.)

```{r eval=FALSE}
requests <- 
  lapply(pages, function(x, query_list) {
    new_query_list <- c(query_list, page = x)
    ## wait to avoid being rate-limited
    Sys.sleep(5)
    r <- GET(nyt_articlesearch_url, query = new_query_list)
  }, query_list = query_list)
```

```{r include=FALSE}
requests <- readRDS("data/modern_love_requests.rds")
```

## Additional packages for tidying data

```{r message=FALSE, warning=FALSE}
library(readr)
library(ggplot2)
library(stringr)
```

## Save responses in JSON format

Not run: write each valid response to a separate json file.

```{r eval=FALSE}
Map(function(r, x) {
  ## build file name
  file_name <- str_c("modern_love", str_pad(as.character(x), 2, pad = "0"), 
                     sep = "_") %>%
    str_c(".json") %>%
    file.path("data", .)
  ## write to file as JSON
  if (status_code(r) == 200) {
    content(r) %>% toJSON() %>% prettify() %>% write_file(file_name)
  }
}, r = requests, x = pages) %>% invisible()
```

## Create data frame with desired fields

One request returned a Status 403; filter that response out.

```{r}
## filter out Status 403 (page 59 from 0 to 60)
status_code(requests[[60]])
requests_ok <- requests[lapply(requests, status_code) == 200]
```

## Create data frame with desired fields

Note the special handling for the author field, discovered by trial and error. One interactive article didn't have a byline.

```{r}
## create data frame
modern_love_from_api <- 
  lapply(requests_ok, function(r) {
    content(r)$response$docs %>%
      lapply(function(x) {
        ## special handling for one interactive feature with a null byline
        ## "Try the 36 Questions on the Way to Love" on 2015-02-13
        author <- if(length(x$byline) > 0) x$byline$original else NA
        data_frame(pub_date = x$pub_date, 
                   title = x$headline$main,
                   author = author,
                   snippet = x$snippet,
                   web_url = x$web_url)
      }) %>% bind_rows()
  }) %>% bind_rows()
```

## Clean data format and byline

```{r}
modern_love_cleaned <- 
  modern_love_from_api %>%
  mutate(pub_date = as.Date(pub_date, format = "%Y-%m-%d"), 
         author = str_replace(author, "^By ", ""))
```

## Check out number of articles per year

```{r echo=FALSE}
modern_love_cleaned %>%
  mutate(pub_year = format(pub_date, "%Y")) %>% 
  ggplot(aes(x = pub_year)) + geom_bar() + theme_classic()
```

## Save data frame to csv

Split columns and podcasts into separate csv files.

`readr` has a dedicated function for writing csv files so that Excel can interpret the encoding of non-ASCII characters correctly. 

```{r}
## simplest way to disambiguate podcasts from regular columns
modern_love_columns <- 
  modern_love_cleaned %>%
  filter(author != "THE NEW YORK TIMES")

modern_love_podcasts <- 
  modern_love_cleaned %>%
  filter(author == "THE NEW YORK TIMES")

## Signal UTF-8 encoding to Excel with `write_excel_csv()`
write_excel_csv(modern_love_columns, "data/modern_love_columns.csv")
write_excel_csv(modern_love_podcasts, "data/modern_love_podcasts.csv")
```